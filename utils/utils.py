import numpy as np
from random import shuffle

class LabelNoise:
    def __init__(self, num_workers, num_classes, gamma=1):
        self.num_workers=num_workers
        self.num_classes=num_classes
        self.gamma=gamma
        self.initialize_confusion_matrix(num_workers, num_classes, gamma)

    def initialize_confusion_matrix(self, num_workers, num_classes, gamma):
        """Initialize confusion matrix for all workers as gamma on the diagonal entries
        and rest is distributed equally"""

        self.confusion_matrix = np.zeros((num_workers,num_classes,num_classes))
        # Set diagonal entries to gamma probability
        for j in range(num_workers):
            self.confusion_matrix[j,:,:] = np.identity(num_classes)*gamma
        # Set non-diogonal entries with equal probability
        self.confusion_matrix[self.confusion_matrix == 0] = (1-gamma)/(num_classes-1)
        return self.confusion_matrix

    def generate_worker_responses(self, x_train, x_test, y_train, y_test):
        """Generates workers responses to the training samples according to their confusion matrices"""
 
        num_train_samples, num_test_samples = x_train.shape[0], x_test.shape[0]
        self.workers_responses = np.zeros((num_train_samples,self.num_workers,self.num_classes)) # variable to store worker respones
        self.workers_train_labels = {} # dictionary to store noisy labels generated by each worker confusion matrices for each training example  
        self.workers_val_labels = {} # dictionary to store true labels for the validation set  

        # dictionary to hold responses of each workers to each sample
        for i in range(self.num_workers):
            self.workers_train_labels['worker' + str(i) + '_label'] = np.zeros((num_train_samples,self.num_classes))   
        
        # iterating over each training example and worker
        for i in range(num_train_samples):
            for j in range(self.num_workers):
                # using the row of the confusion matrix corresponding to the true label generating the noisy label
                temp_rand = np.random.multinomial(1,self.confusion_matrix[j,y_train[i],:])
                # storing the noisy label in the resp variable 
                self.workers_responses[i,j,:] = temp_rand
                # storing the noisy label in the dictionary
                self.workers_train_labels['worker' + str(j) + '_label'][i] = temp_rand
        
        self.workers_val_labels['worker' + str(0) + '_label'] = np.zeros((num_test_samples,self.num_classes))  
        # storing the true labels of the examples in the validation set in the dictionary
        for i in range(num_test_samples):
            self.workers_val_labels['worker' + str(0) + '_label'][i][int(y_test[i])] = 1
        
        # returning the noisy responses of the workers stored in the resp numpy array, 
        # the noisy labels stored in the dictionary that is used by the deep learning module
        # the true lables of the examples in the validation set stored in the dictionary
        return self.workers_responses, self.workers_train_labels, self.workers_val_labels

class LabelDenoise:
    def majority_voting(workers_responses):
        # computes majority voting label
        # ties are broken uniformly at random
        num_samples = workers_responses.shape[0]
        num_classes = workers_responses.shape[2]
        pred_mv = np.zeros((num_samples), dtype = np.int)
        for i in range(num_samples):
            # finding all labels that have got maximum number of votes
            poss_pred = np.where(np.sum(workers_responses[i],0) == np.max(np.sum(workers_responses[i],0)))[0]
            shuffle(poss_pred)
            # choosing a label randomly among all the labels that have got the highest number of votes
            pred_mv[i] = poss_pred[0]   
        pred_mv_vec = np.zeros((num_samples,num_classes))
        # returning one-hot representation of the majority vote label
        pred_mv_vec[np.arange(num_samples), pred_mv] = 1
        return pred_mv_vec

    def post_prob_DS(workers_responses,estimated_classes):
        # computes posterior probability distribution of the true label given the noisy labels annotated by the workers
        # and model prediction
        num_samples = workers_responses.shape[0]
        num_workers = workers_responses.shape[1]
        num_classes = workers_responses.shape[2]
        
        tmp_classes = np.zeros((num_samples,num_classes))
        estimated_conf_matrix = np.zeros((num_workers,num_classes,num_classes))
        tmp_conf_matrix = np.zeros((num_workers,num_classes,num_classes))
        
        #Estimating confusion matrices of each worker by assuming model prediction "e_class" is the ground truth label
        for i in range(num_samples):
            for j in range(num_workers):
                tmp_conf_matrix[j,:,:] = tmp_conf_matrix[j,:,:] + np.outer(estimated_classes[i],workers_responses[i,j])
        #regularizing confusion matrices to avoid numerical issues
        for j in range(num_workers):  
            for r in range(num_classes):
                if (np.sum(tmp_conf_matrix[j,r,:]) ==0):
                    # assuming worker is spammer for the particular class if there is no estimation for that class for that worker
                    tmp_conf_matrix[j,r,:] = 1/num_classes
                else:
                    # assuming there is a non-zero probability of each worker assigning labels for all the classes
                    tmp_conf_matrix[j,r,:][tmp_conf_matrix[j,r,:]==0] = 1e-10
            estimated_conf_matrix[j,:,:] = np.divide(tmp_conf_matrix[j,:,:],np.outer(np.sum(tmp_conf_matrix[j,:,:],axis =1),np.ones(num_classes)))
        # Estimating posterior distribution of the true labels using confusion matrices of the workers and the original
        # noisy labels annotated by the workers
        for i in range(num_samples):
            for j in range(num_workers): 
                if (np.sum(workers_responses[i,j]) ==1):
                    tmp_classes[i] = tmp_classes[i] + np.log(np.dot(estimated_conf_matrix[j,:,:],np.transpose(workers_responses[i,j])))
            tmp_classes[i] = np.exp(tmp_classes[i])
            tmp_classes[i] = np.divide(tmp_classes[i],np.outer(np.sum(tmp_classes[i]),np.ones(num_classes)))
            estimated_classes[i] = tmp_classes[i]           
        return estimated_classes